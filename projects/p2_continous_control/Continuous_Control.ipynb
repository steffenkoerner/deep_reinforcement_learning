{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "This notebook is just used to show the performance ot the trained model. To train the model and see the whole code please look\n",
    "at the Continuous_Control.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ReplayBuffer import ReplayBuffer, GaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 128\n",
    "TAU = 1e-3 \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = 400, fc2_units = 300):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size + action_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1) # Needs to be 1 as this is the max(Q(s,a)) that is learned\n",
    "        self.load_state_dict(torch.load('final_weight_critic.pth'))\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = 400, fc2_units = 300):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.load_state_dict(torch.load('final_weight_actor.pth'))\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class DDPGNetwork():\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        self.actor_network = ActorNetwork(state_size, action_size, seed).to(device)\n",
    "        self.critic_network = CriticNetwork(state_size, action_size, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=LEARNING_RATE)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_network.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "\n",
    "    def actor(self, state):\n",
    "        return self.actor_network(state)\n",
    "    \n",
    "    def critic(self, states,actions):\n",
    "        return self.critic_network(torch.cat((states, actions), 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDPGAgent():\n",
    "    def __init__(self, state_size, action_size, seed, warmup = 100):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.action_lowest_value = -1\n",
    "        self.action_highest_value = 1\n",
    "        self.warump = warmup\n",
    "        self.gaussian_noise = GaussianNoise(size=action_size, std_start=0.2, std_end=0.01,steps=1000) \n",
    "\n",
    "        # DDPG-Network\n",
    "        self.local_network = DDPGNetwork(state_size, action_size, seed)\n",
    "        self.target_network = DDPGNetwork(state_size, action_size, seed)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size=action_size, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, seed=seed)   \n",
    "        \n",
    "    def get_acion_per_current_policy_for(self, state):   \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.local_network.actor_network.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.local_network.actor(state).cpu().data.numpy()\n",
    "        self.local_network.actor_network.train()\n",
    "        actions = np.clip(actions, self.action_lowest_value, self.action_highest_value)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 35.77999920025468\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = brain.vector_observation_space_size\n",
    "\n",
    "agent = DDPGAgent(state_size= state_size, action_size = action_size, seed = 0 , warmup = 10)\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] \n",
    "state = env_info.vector_observations[0] \n",
    "score = 0                                         # initialize the score (for each agent)\n",
    "while True:\n",
    "    action = agent.get_acion_per_current_policy_for(state)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    if done:\n",
    "        break \n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
