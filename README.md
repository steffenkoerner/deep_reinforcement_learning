
# Deep Reinforcement Learning

Deep learning is a type of [machine learning](https://en.wikipedia.org/wiki/Machine_learning), which is a type of [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence).

Combining Deep Learning with Reinforcement Learning Algorithm leads to Deep Reinforcement Learning. This repository contains my learning progress in Deep Reinforcement Learning.

## Table of Contents

### Projects

The projects that are contained in this repro can be found below.  All of the projects use rich simulation environments from [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents).

* [Navigation](https://github.com/steffenkoerner/deep_reinforcement_learning/tree/main/projects/p1_navigation): This projects trains an agent to collect yellow bananas while avoiding blue bananas.
* [REINFORCE Algorithm](https://github.com/steffenkoerner/deep_reinforcement_learning/tree/main/projects/reinforce_algorithm): This project contains the implementation of the REINFORCE algorithm to solve the OpenAI CartPole environment

### Additinal Resources

* [Cheatsheet](https://github.com/steffenkoerner/deep_reinforcement_learning/blob/main/cheatsheet.pdf): Contains a summary of all important reinforcement learning algorithms

### Deep Reinforcement Learning
- [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)
- [Write an AI to win at Pong from scratch with Reinforcment Learning](https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0)
- [Conquering OpenAI Retro Contest 2: Demistifying Rainbow Baseline](https://medium.com/intelligentunit/conquering-openai-retro-contest-2-demystifying-rainbow-baseline-9d8dd258e74b)
- [Request for Research 2.0 OpenAI](https://openai.com/blog/requests-for-research-2/#cartpole)

## Research Papers
- [Dueling Network Architectures for DRL](https://arxiv.org/abs/1511.06581)
- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)
- [DRL with Double Q Learning](https://arxiv.org/abs/1509.06461)
- [Issues in Using Function Approximation in DRL](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf)
- [Asynchronous Methods for DRL](https://arxiv.org/abs/1602.01783)
- [A Distributional Perspective on RL](https://arxiv.org/abs/1707.06887)
- [Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)
- [Rainbow: Combining Improvements in DRL](https://arxiv.org/abs/1710.02298)




### Evolution Strategies
- [Evolution Strategies as a Scalable Alernative to Reinforcement Learning](https://openai.com/blog/evolution-strategies/)



### Neural Networks
- [Deep Learning Book (Ian Goodfellow)](http://www.deeplearningbook.org/)
- [Hackers Guide to Neural Networks](http://karpathy.github.io/neuralnets/)
- [Online Book Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)
- [Deep Learning Papers Reading Roadmap](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap)
- [Google Crash Course](https://developers.google.com/machine-learning/crash-course/fitter/graph)
- [Neural Network Playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.30786&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
- [Rules of Machine Learning](https://developers.google.com/machine-learning/guides/rules-of-ml/)
- [Free Udacity Course Deep Learning](https://eu.udacity.com/course/deep-learning--ud730?utm_medium=referral&utm_campaign=api)
- [Github code for applying style to images](https://github.com/lengstrom/fast-style-transfer)
- [An Insiderâ€™s Guide to Keeping Up with the AI Experts](https://blog.udacity.com/2018/09/machine-learning-ai-experts-on-twitter.html)
- [Understanding the difficulty of training deep feedforward neural networks](jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v2.pdf)
- [Tips and Tricks](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)



#### Backpropagation
- [Summary Backpropagation](https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/)
- [Backpropagation intuitve](http://colah.github.io/posts/2015-08-Backprop/)
- [Backpropagation in detail](http://neuralnetworksanddeeplearning.com/chap2.html)
- [Karpathy - Yes, you should understand backpropagation](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
- [CS231n Winter 2016 Lecture 4 Backpropagation](https://www.youtube.com/watch?v=59Hbtz7XgjM)

### CNN
- [A easy introduction to CNN (youtube)](https://www.youtube.com/watch?v=2-Ol7ZB0MmU)
- [Convolutional Neuronal Network (CS231)](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
- [How CNN's see the world](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)
- [What a neural net see](https://experiments.withgoogle.com/what-neural-nets-see)
- [Visualizing and Understanding Deep Neural Networks](https://www.youtube.com/watch?v=ghEmQSxT6tw&t=5s)
- [Deep Visualization Toolbox](https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=78s)

### Capsule Networks
- [Blogpost Capsule Networks](https://cezannec.github.io/Capsule_Networks/)

### LSTM
- [Understanding LSTM ](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Exploring LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/)
- [CS231n Lecture 10 - Recurrent Neural Networks, Image Captioning, LSTM](https://www.youtube.com/watch?v=iX5V1WpxxkY)
- [Tutorial LSTM](https://skymind.ai/wiki/lstm)
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [How to Construct Deep Recurrent Neural Networks](https://arxiv.org/abs/1312.6026)

### Generative Adversarial Networks
- [Original Paper for GAN's](https://arxiv.org/abs/1406.2661)
- [Cycle GAN](https://arxiv.org/pdf/1703.10593.pdf)
- [LSGAN](https://arxiv.org/pdf/1611.04076.pdf)
 
 ### Optimization
 - [An overview of gradient descent optimization algorithm](https://ruder.io/optimizing-gradient-descent/)
#### Examples
- [Pix2Pix](https://affinelayer.com/pixsrv/)
- [Cycle GAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)


### Competitions
- [kaggle](https://www.kaggle.com/)

### Games
- [Flappy Bird(Github)](https://github.com/yenchenlin/DeepLearningFlappyBird)

### Machine Learning Projects
- [30 Amazing Machine Learning Projects](https://medium.mybridge.co/30-amazing-machine-learning-projects-for-the-past-year-v-2018-b853b8621ac7)

### Interview Machine Learning
- [What I am learned from AI interviews](https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc)


### Other Links
- [50 best public datasets for machine learning](https://towardsdatascience.com/the-50-best-public-datasets-for-machine-learning-d80e9f030279)



 ### Study Plan
 - [Study Plan for Machine Learning](https://github.com/ZuzooVn/machine-learning-for-software-engineers)
 - [Study Plan for Software Engineer](https://github.com/jwasham/coding-interview-university)
   - [10 Operating System Concepts to know](https://medium.com/cracking-the-data-science-interview/the-10-operating-system-concepts-software-developers-need-to-remember-480d0734d710)

## Unordered Content (Needs Clean Up !!!)
### Concept a ML Engineer must know
- Supervised Learning
- Unsupervised Learning
- Reinforcement Learning
- Backpropagation
- Gradient Decent, Stochastic Gradient Descent
  - Local Minima
  - Momentum
- Capsule Network
- Autoencoder
- Neural Network, MLP, CNN, RNN
- CNN
  - Convoluational Layer
  - Transposed Convolutional Layer
  - Pooling Layer
  - Padding
    - Adding border pixels around the image
  - Stride
- Regularization (L1, L2, Dropout)
- Vanishing Gradient
- Activation Function (Sigmoid, Relu,...)
- Learning Rate
- [Transfer Learning](http://cs231n.github.io/transfer-learning/)
  - use pre-trained networks to improve the performance of your classifier
  
  
  ### Skills a ML should have
- Building and training neural networks
- Model evaluation and validation
- Convolutional neural networks
- Autoencoders and feature extraction
- Transfer learning
- Recurrent neural networks
- Natural language processing
- Data augmentation
- Generative adversarial networks
- Hyperparameter tuning
- Model deployment and serving
    

### Brain Storming

## Steps to do deep learning
- Visualize Data
- Pre-Process Data (Filtering of bad examples, Appyling little changes, Normalizing,...)
- Define a Model
- Train the model (Defining loss and optimization functions)
- Save the best model


Unordered Topics:


Introduction

What is machine learning?

Typical tasks in ML

k-Nearest neighbors

kNN for classification and regression

Distance functions

Curse of dimensionality

Decision trees

Constructing & pruning decision trees

Basics of information theory

Probabilistic inference

Parameter estimation

Maximum likelihood principle

Maximum a posteriori

Full Bayesian approach

Linear regression

Linear basis function models

Overfitting

Bias-variance tradeoff

Model selection

Regularization

Linear classification

Perceptron algorithm

Generative / discriminative models for classification

Linear discriminant analysis

Logistic regression

Optimization

Gradient-based methods

Convex optimization

Stochastic gradient descent

Deep learning

Feedforward neural networks

Backpropagation

Structured data: CNNs, RNNs

Training strategies

Frameworks

Advanced architectures

Support vector machines

Maximum margin classification

Soft-margin SVM

Kernel methods

Kernel trick

Kernelized linear regression

Dimensionality reduction

Principal component analysis

Singular value decomposition

Probabilistic PCA

Matrix factorization

Autoencoders

Clustering

k-means

Gaussian mixture models

EM algorithm
